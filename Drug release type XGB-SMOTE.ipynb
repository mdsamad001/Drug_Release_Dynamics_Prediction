{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15c3cd8e-5df7-4628-bf3f-8d8bfd2f795b",
   "metadata": {},
   "source": [
    "# Prediction of drug release type with XGBoost with synthetic minority over sampling technique (XGB-SMOTE)\n",
    "## Initialization of environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fd73ce7-7966-4def-86e7-26908bc6689d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, roc_auc_score, precision_score,\n",
    "    recall_score, f1_score, confusion_matrix\n",
    ")\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from optuna.samplers import TPESampler\n",
    "import warnings\n",
    "optuna.logging.set_verbosity(optuna.logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66277699-222b-48bc-a945-d1f2f95e4894",
   "metadata": {},
   "source": [
    "## Data loading and preparation\n",
    "Definition of variables, data loading, normalization and interpolation of the drug release profile, calculation of drug release profile AUC, and definition of drug release type (burst: AUC > 0.5, delayed: AUC <= 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b529468-aa6e-4fc4-aca5-bebc154d23ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_interp_pts = 11\n",
    "n_outer_folds = 10\n",
    "n_inner_folds = 2\n",
    "n_trials = 50\n",
    "# ----------------------------------------------------------------------------------------\n",
    "# Load data\n",
    "# ----------------------------------------------------------------------------------------\n",
    "file_path_form = 'mp_dataset_processed_no_dupes.xlsx'\n",
    "file_path_time = 'mp_dataset_processed_time_release_only.xlsx'\n",
    "formulation_df = pd.read_excel(file_path_form, engine='openpyxl')\n",
    "release_df = pd.read_excel(file_path_time, engine='openpyxl')\n",
    "# ----------------------------------------------------------------------------------------\n",
    "# Encode categorical\n",
    "# ----------------------------------------------------------------------------------------\n",
    "unique_values_emulsion = formulation_df['Formulation Method'].unique()\n",
    "mapping = {v: i for i, v in enumerate(unique_values_emulsion)}\n",
    "formulation_df['Formulation Method Encoded'] = formulation_df['Formulation Method'].map(mapping)\n",
    "formulation_df.drop(columns=['Formulation Method', 'Drug SMILES'], inplace=True)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------------\n",
    "# Interpolation\n",
    "# ----------------------------------------------------------------------------------------\n",
    "group = release_df.groupby('Formulation Index')['Time']\n",
    "min_time = group.transform('min')\n",
    "max_time = group.transform('max')\n",
    "release_df['Normalized Time'] = (release_df['Time'] - min_time) / (max_time - min_time)\n",
    "normalized_times = np.linspace(0, 1, num_interp_pts)\n",
    "interpolated_dfs = []\n",
    "for formulation, g in release_df.groupby('Formulation Index'):\n",
    "    g = g.sort_values('Time')\n",
    "    time_min, time_max = g['Time'].min(), g['Time'].max()\n",
    "    g['Normalized Time'] = (g['Time'] - time_min) / (time_max - time_min)\n",
    "    interp_release = np.interp(normalized_times, g['Normalized Time'], g['Release'])\n",
    "    interpolated_dfs.append(pd.DataFrame({\n",
    "        'Formulation Index': formulation,\n",
    "        'Normalized Time': normalized_times,\n",
    "        'Interpolated Release': interp_release\n",
    "    }))\n",
    "interp_df = pd.concat(interpolated_dfs, ignore_index=True)\n",
    "\n",
    "X = formulation_df.drop(columns=['Formulation Index']).to_numpy()\n",
    "#X = formulation_df.to_numpy()  # [321, 11]\n",
    "groups = interp_df.groupby('Formulation Index')['Interpolated Release']\n",
    "\n",
    "\n",
    "auc = (\n",
    "    interp_df.groupby(\"Formulation Index\")\n",
    "      .apply(lambda g: np.trapz(g[\"Interpolated Release\"], g[\"Normalized Time\"]))\n",
    "      .reset_index(name=\"AUC\")\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------------------------------------\n",
    "# AUC and drug release type definition\n",
    "# ----------------------------------------------------------------------------------------\n",
    "auc['burst'] = (auc['AUC'] > 0.5).astype(int)\n",
    "y = auc['burst'].values  # shape (n_samples,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4b40d4-88c5-4d75-98ae-aa6165ac0427",
   "metadata": {},
   "source": [
    "## Model Definition and Training\n",
    "Nested cross-validation with optuna hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11d308ff-f06c-40f7-917a-fe6527801c39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Outer Fold 1\n",
      "Best params: {'max_depth': 3, 'learning_rate': 0.2774520656659327, 'n_estimators': 270, 'subsample': 0.9385800061142205, 'colsample_bytree': 0.6641923018346889, 'gamma': 0.49716544492757214, 'reg_alpha': 0.6748391024432011, 'reg_lambda': 0.4197094246892561}\n",
      "Fold 1 - ACC: 0.76, AUC: 0.58, Prec: 0.81, Rec: 0.88, F1: 0.84\n",
      "\n",
      "Outer Fold 2\n",
      "Best params: {'max_depth': 12, 'learning_rate': 0.25109835006527126, 'n_estimators': 230, 'subsample': 0.9228346746093398, 'colsample_bytree': 0.6039445305934279, 'gamma': 0.31303061748141814, 'reg_alpha': 0.4375758355676372, 'reg_lambda': 0.8234685018114029}\n",
      "Fold 2 - ACC: 0.72, AUC: 0.86, Prec: 0.77, Rec: 0.87, F1: 0.82\n",
      "\n",
      "Outer Fold 3\n",
      "Best params: {'max_depth': 12, 'learning_rate': 0.10097589562081635, 'n_estimators': 279, 'subsample': 0.5053504887041067, 'colsample_bytree': 0.9575677122021107, 'gamma': 0.35253511671493304, 'reg_alpha': 0.06620321470734572, 'reg_lambda': 0.21855794582599203}\n",
      "Fold 3 - ACC: 0.69, AUC: 0.79, Prec: 0.84, Rec: 0.7, F1: 0.76\n",
      "\n",
      "Outer Fold 4\n",
      "Best params: {'max_depth': 8, 'learning_rate': 0.13832956670211233, 'n_estimators': 219, 'subsample': 0.9754190474771028, 'colsample_bytree': 0.8549441424244961, 'gamma': 1.2918665774424907, 'reg_alpha': 0.18359469756177804, 'reg_lambda': 0.9909797455370998}\n",
      "Fold 4 - ACC: 0.88, AUC: 0.86, Prec: 0.95, Rec: 0.87, F1: 0.91\n",
      "\n",
      "Outer Fold 5\n",
      "Best params: {'max_depth': 12, 'learning_rate': 0.11246980603765391, 'n_estimators': 51, 'subsample': 0.788474586944839, 'colsample_bytree': 0.729665686278495, 'gamma': 2.0541415383401858, 'reg_alpha': 0.20461445685727536, 'reg_lambda': 0.2748529489952627}\n",
      "Fold 5 - ACC: 0.78, AUC: 0.82, Prec: 0.87, Rec: 0.83, F1: 0.85\n",
      "\n",
      "Outer Fold 6\n",
      "Best params: {'max_depth': 15, 'learning_rate': 0.1728712058216421, 'n_estimators': 254, 'subsample': 0.8704222583697268, 'colsample_bytree': 0.8597813422278708, 'gamma': 0.28836321062338066, 'reg_alpha': 0.5173500465610804, 'reg_lambda': 0.5451467998789583}\n",
      "Fold 6 - ACC: 0.88, AUC: 0.91, Prec: 0.88, Rec: 0.96, F1: 0.92\n",
      "\n",
      "Outer Fold 7\n",
      "Best params: {'max_depth': 3, 'learning_rate': 0.2464838142519019, 'n_estimators': 227, 'subsample': 0.8645035840204937, 'colsample_bytree': 0.8856351733429728, 'gamma': 0.3702232586704518, 'reg_alpha': 0.3584657285442726, 'reg_lambda': 0.11586905952512971}\n",
      "Fold 7 - ACC: 0.78, AUC: 0.88, Prec: 0.81, Rec: 0.92, F1: 0.86\n",
      "\n",
      "Outer Fold 8\n",
      "Best params: {'max_depth': 4, 'learning_rate': 0.280605018914862, 'n_estimators': 106, 'subsample': 0.8672355932798954, 'colsample_bytree': 0.8984779365635164, 'gamma': 1.0800328636299672, 'reg_alpha': 0.17230482402691852, 'reg_lambda': 0.6291973312070054}\n",
      "Fold 8 - ACC: 0.72, AUC: 0.68, Prec: 0.83, Rec: 0.79, F1: 0.81\n",
      "\n",
      "Outer Fold 9\n",
      "Best params: {'max_depth': 16, 'learning_rate': 0.13540257068954883, 'n_estimators': 187, 'subsample': 0.740589590172496, 'colsample_bytree': 0.5030909259374474, 'gamma': 2.4154632456214107, 'reg_alpha': 0.03665562146067878, 'reg_lambda': 0.5791113009255469}\n",
      "Fold 9 - ACC: 0.81, AUC: 0.88, Prec: 0.91, Rec: 0.83, F1: 0.87\n",
      "\n",
      "Outer Fold 10\n",
      "Best params: {'max_depth': 14, 'learning_rate': 0.04170197716367252, 'n_estimators': 64, 'subsample': 0.9253780951591563, 'colsample_bytree': 0.6177937223335518, 'gamma': 0.5256166727406284, 'reg_alpha': 0.9985855868230558, 'reg_lambda': 0.6388606655956144}\n",
      "Fold 10 - ACC: 0.66, AUC: 0.7, Prec: 0.81, Rec: 0.71, F1: 0.76\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------------\n",
    "# Model wrapper for XGB-SMOTE classification\n",
    "# ----------------------------------------------------------------------------------------\n",
    "class XGBoostModel:\n",
    "    def __init__(self, **params):\n",
    "        self.model = xgb.XGBClassifier(**params, random_state=42, eval_metric='logloss')\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.model.predict_proba(X)[:, 1]\n",
    "\n",
    "# ----------------------------------------------------------------------------------------\n",
    "# Storage\n",
    "# ----------------------------------------------------------------------------------------\n",
    "stored_best_models = []\n",
    "stored_test_targets = []\n",
    "stored_best_preds = []\n",
    "stored_best_proba_all = []\n",
    "stored_metrics = []  \n",
    "\n",
    "# ----------------------------------------------------------------------------------------\n",
    "# Nested CV setup\n",
    "# ----------------------------------------------------------------------------------------\n",
    "outer_kf = StratifiedKFold(n_splits=n_outer_folds, shuffle=True, random_state=42)\n",
    "inner_kf = StratifiedKFold(n_splits=n_inner_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# ----------------------------------------------------------------------------------------\n",
    "# Outer CV loop\n",
    "# ----------------------------------------------------------------------------------------\n",
    "for outer_fold, (train_val_idx, test_idx) in enumerate(outer_kf.split(X, y)):\n",
    "    print(f\"\\nOuter Fold {outer_fold + 1}\")\n",
    "\n",
    "    X_train_val, y_train_val = X[train_val_idx], y[train_val_idx]\n",
    "    X_test, y_test = X[test_idx], y[test_idx]\n",
    "    stored_test_targets.append(y_test)\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------\n",
    "    # Optuna objective for inner CV\n",
    "    # ----------------------------------------------------------------------------------------\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 20),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 300),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "            \"gamma\": trial.suggest_float(\"gamma\", 0, 5),\n",
    "            \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0, 1),\n",
    "            \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0, 1),\n",
    "        }\n",
    "\n",
    "        val_accs = []\n",
    "\n",
    "        for inner_train_idx, inner_val_idx in inner_kf.split(X_train_val, y_train_val):\n",
    "\n",
    "            X_tr, X_val = X_train_val[inner_train_idx], X_train_val[inner_val_idx]\n",
    "            y_tr, y_val = y_train_val[inner_train_idx], y_train_val[inner_val_idx]\n",
    "\n",
    "            # ----------------------\n",
    "            # Apply SMOTE to inner training set only\n",
    "            # ----------------------\n",
    "            smote = SMOTE(random_state=42)\n",
    "            X_tr_res, y_tr_res = smote.fit_resample(X_tr, y_tr)\n",
    "            model = XGBoostModel(**params)\n",
    "            model.fit(X_tr_res, y_tr_res)\n",
    "            preds_val = model.predict(X_val)\n",
    "            val_accs.append(accuracy_score(y_val, preds_val))\n",
    "\n",
    "        return 1 - np.mean(val_accs)\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------\n",
    "    # Run Optuna study\n",
    "    # ----------------------------------------------------------------------------------------\n",
    "    tpe_sampler = TPESampler(seed=42) \n",
    "    study = optuna.create_study(direction=\"minimize\", sampler=tpe_sampler)\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    best_params = study.best_params\n",
    "    print(\"Best params:\", best_params)\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------\n",
    "    # Train best model on full outer training data with SMOTE\n",
    "    # ----------------------------------------------------------------------------------------\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_val_res, y_train_val_res = smote.fit_resample(X_train_val, y_train_val)\n",
    "\n",
    "    best_model = XGBoostModel(**best_params)\n",
    "    best_model.fit(X_train_val_res, y_train_val_res)\n",
    "    preds_best = best_model.predict(X_test)\n",
    "    preds_best_proba = best_model.predict_proba(X_test)\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------\n",
    "    # Compute metrics\n",
    "    # ----------------------------------------------------------------------------------------\n",
    "    acc_best = accuracy_score(y_test, preds_best)\n",
    "    auc_best = roc_auc_score(y_test, preds_best_proba)\n",
    "    precision_best = precision_score(y_test, preds_best, zero_division=0)\n",
    "    recall_best = recall_score(y_test, preds_best, zero_division=0)\n",
    "    f1_best = f1_score(y_test, preds_best, zero_division=0)\n",
    "\n",
    "    try:\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, preds_best).ravel()\n",
    "        specificity_best = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "    except ValueError:\n",
    "        specificity_best = 0.0\n",
    "\n",
    "    # Store results\n",
    "    stored_best_models.append(best_model)\n",
    "    stored_best_preds.append(preds_best)\n",
    "    stored_best_proba_all.append(preds_best_proba)\n",
    "    stored_metrics.append({\n",
    "        \"fold\": outer_fold + 1,\n",
    "        \"accuracy\": acc_best,\n",
    "        \"auc\": auc_best,\n",
    "        \"precision\": precision_best,\n",
    "        \"recall_sensitivity\": recall_best,\n",
    "        \"f1\": f1_best\n",
    "    })\n",
    "\n",
    "    print(f\"Fold {outer_fold+1} - ACC: {acc_best:.2}, AUC: {auc_best:.2}, \"\n",
    "          f\"Prec: {precision_best:.2f}, Rec: {recall_best:.2}, F1: {f1_best:.2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847d762e-bf8a-4dae-906f-2fc3953a15b7",
   "metadata": {},
   "source": [
    "## Performance metrics\n",
    "Accuracy, AUC, precision, recall sensitivity, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2050173c-c06f-488c-9cca-fd08ada1dc40",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Nested CV Results:\n",
      "ACCURACY: 0.77 ± 0.07\n",
      "AUC: 0.80 ± 0.11\n",
      "PRECISION: 0.85 ± 0.06\n",
      "RECALL_SENSITIVITY: 0.84 ± 0.08\n",
      "F1: 0.84 ± 0.06\n"
     ]
    }
   ],
   "source": [
    "# ----------------------\n",
    "# Save metrics\n",
    "# ----------------------\n",
    "metrics_df = pd.DataFrame(stored_metrics)\n",
    "#metrics_df.to_csv(\"RF_class_metrics.csv\", index=False)\n",
    "#print(\"\\nRF_classn_metrics.csv\")\n",
    "\n",
    "# ----------------------\n",
    "# Final summary\n",
    "# ----------------------\n",
    "print(\"\\nFinal Nested CV Results:\")\n",
    "for metric in [\"accuracy\", \"auc\", \"precision\", \"recall_sensitivity\", \"f1\"]:\n",
    "    print(f\"{metric.upper()}: {metrics_df[metric].mean():.2f} ± {metrics_df[metric].std():.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:karla_env]",
   "language": "python",
   "name": "conda-env-karla_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
